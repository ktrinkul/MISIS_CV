# MISIS_CV
Практика по курсу CV

# HW1
#### ResNet18 (обучен на CIFAR10) и сравнение с простым CNN  
Статья про ResNet: https://arxiv.org/pdf/1512.03385  
Cтруктура ResNet: https://medium.com/@ibtedaazeem/understanding-resnet-architecture-a-deep-dive-into-residual-neural-network-2c792e6537a9  
Итог: ResNet18 занимает больше времени на обучение и валидацию, чем CNN. ResNet18 лучше справляется с уменьшением loss-а и на тестовой и на валидационной выборке, а также почти в 1,5 раза увеличивает accuracy в сравнении с CNN.

# HW2
#### Тренировка модели сегментации U-Net с датасетом Oxford-IIIT Pet.  
Проведен перебор гиперпаметров.    
Итог_1:
- при меньшем lr плавнее результаты, тк идет постепенное обновление весов, без резких скачков, хоть на обучение и будет тратиться больше времени
- при больших батчах (32) результаты менее стабильны, особенно на валидационных данных, чем при меньших размерах батчей (8)  
Проверена идея по модификации U-Net: добавление свертки (вместо двух раз - три).  
Итог_2:  
- предсказание некорректно так как значение IOU < 0.5 ("В COCO и других стандартах IoU ≥ 0.5 используется как порог для классификации предсказаний")
- на Predicted Mask видны очертания фона, особенно если резкие границы (на стенах) - но инфы о нет в Ground Truth
- модель не улучшает свои значения в течение всего обучения
Возможные причины:  
- снижение разрешения при дополнительных свёртках -> потеря мелких деталей, нужных для сегментации, так как сразу использовано несколько свероток - взяты более глубокие признаки и пропущены более явные
- обучение с epoches = 10 - возможно маленькое колво эпох для усложненной модели
(сделать глубже, добавить нормализацию слоев, поменять бекбон на другой, и тд). Также сделать 1-2 графика с результатами. После полезно написать мысли, почему получились такие результаты

# HW3
#### Реализация Variance-Preserving SDE диффузии (линейное расписание и косинусовое)  
SDE: https://arxiv.org/pdf/2011.13456  
iDDPM: https://arxiv.org/pdf/2102.09672  
Разные варианты перезвешивание лосса от времени.  
Итог_1:  
Использование взвешенной функции потерь приводит к более плавному изменению значения лосса, чем при обычном подсчете. Это снижает вероятность резких изменений в весах модели во время обучения, обеспечивая более стабильный и постепенный процесс улучшения

# HW4
#### Semantic Segmentation: Encoder-Decoder vs Transformers  
Классическая модель для сегментации (U-Net) и трансформер для сегментации (segformer). Оценка разницы в затратах времени и памяти на обучение (с учетом количество параметров).   
Итог_1:   
Трансформеры круче классических моделей. Трансформеры при равных с обычными сегментационными моделями бранчах, lerning rate, оптимизаторах, данных и других параматерах работают эффективнее - одна эпоха занимает меньше времени и меньше памяти, чем, например, модель UNet.


